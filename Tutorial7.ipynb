{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Depth Map from Stereo Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load left and right images\n",
    "SAMPLES_DATA_DIR = 'C:/opencv/sources/samples/data/'\n",
    "# Load and downscale images for faster processing (M×N  image becomes M/2×N/2 image)\n",
    "imgL = cv2.pyrDown( cv2.imread(SAMPLES_DATA_DIR+'aloeL.jpg') )\n",
    "imgR = cv2.pyrDown( cv2.imread(SAMPLES_DATA_DIR+'aloeR.jpg') )\n",
    "\n",
    "    \n",
    "# Create an StereoMatcher object using semi-global block matching algorithm\n",
    "# minDisparity: Minimum possible disparity value\n",
    "# numDisparities: Maximum disparity minus minimum disparity\n",
    "# blockSize: Matched block size\n",
    "# P1: The first parameter controlling the disparity smoothness\n",
    "# P2: The second parameter controlling the disparity smoothness\n",
    "# P1 is the penalty on the disparity change by plus or minus 1 between neighbor pixels\n",
    "# P2 is the penalty on the disparity change by more than 1 between neighbor pixels\n",
    "# disp12MaxDiff: Max allowed difference (in integer pixel units) in the left-right disparity check\n",
    "# uniquenessRatio: % by which the minimum computed cost function value should win”the second best value \n",
    "min_disp = 0\n",
    "window_size = 3\n",
    "num_disp = 112-min_disp\n",
    "p1 = 8*3*window_size**2\n",
    "p2 = 32*3*window_size**2\n",
    "stereo = cv2.StereoSGBM_create(minDisparity = min_disp, numDisparities = num_disp,\n",
    "        blockSize = 16, P1 = p1, P2 = p2,\n",
    "        disp12MaxDiff = 1, uniquenessRatio = 10)\n",
    "\n",
    "# Compute the disparity map\n",
    "disparity = stereo.compute(imgL,imgR)\n",
    "\n",
    "# Display\n",
    "plt.subplot(1,2,1),plt.imshow(imgL),plt.xticks([]), plt.yticks([]),plt.title('left image')\n",
    "plt.subplot(1,2,2),plt.imshow(imgR),plt.xticks([]), plt.yticks([]),plt.title('right image')\n",
    "plt.show()\n",
    "plt.imshow(disparity,'hot'),plt.xticks([]), plt.yticks([]),plt.title('disparity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Epipolar Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load left and right images\n",
    "SAMPLES_DATA_DIR = 'C:/opencv/sources/samples/data/'\n",
    "imgL = cv2.imread(SAMPLES_DATA_DIR+'left08.jpg',cv2.IMREAD_GRAYSCALE)  #queryimage # left image\n",
    "imgR = cv2.imread(SAMPLES_DATA_DIR+'right08.jpg',cv2.IMREAD_GRAYSCALE) #trainimage # right image\n",
    "\n",
    "# We need to find as many possible matches between two images to find the fundamental matrix\n",
    "# We'll use SIFT features\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# create a Brute-Force Matcher\n",
    "bfMatcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kpL, desL = sift.detectAndCompute(imgL,None)\n",
    "kpR, desR = sift.detectAndCompute(imgR,None)\n",
    "\n",
    "# Match descriptors (find the best two matches for each descriptor)\n",
    "matches = bfMatcher.knnMatch(desL,desR,k=2)\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "# We'll store the list of best matches from both the images\n",
    "good = []\n",
    "ptsL = []\n",
    "ptsR = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.6*n.distance:\n",
    "        good.append(m)\n",
    "        ptsR.append(kpR[m.trainIdx].pt)\n",
    "        ptsL.append(kpL[m.queryIdx].pt)\n",
    "        \n",
    "# Find the Fundamental Matrix\n",
    "ptsL = np.int32(ptsL)\n",
    "ptsR = np.int32(ptsR)\n",
    "F, mask = cv2.findFundamentalMat(ptsL,ptsR,cv2.FM_LMEDS)\n",
    "\n",
    "# mask: every element of which is set to 0 for outliers and to 1 for the other points\n",
    "# We select only inlier points\n",
    "ptsL = ptsL[mask.ravel()==1]\n",
    "ptsR = ptsR[mask.ravel()==1]\n",
    "\n",
    "\n",
    "# helper function to draw epilines\n",
    "# Epilines corresponding to the points in first image is drawn on second image\n",
    "def drawlines(img1,img2,lines,pts1,pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    # img1 shpae\n",
    "    r,c = img1.shape\n",
    "    # convert from grayscale to BGR\n",
    "    img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)\n",
    "    img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)\n",
    "    # for each line\n",
    "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
    "        # pick a random color\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        # \n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        # draw the line and the points\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,1)\n",
    "        img1 = cv2.circle(img1,tuple(pt1),5,color,-1)\n",
    "        img2 = cv2.circle(img2,tuple(pt2),5,color,-1)\n",
    "    return img1,img2\n",
    "\n",
    "# Find the epilines\n",
    "# Epilines corresponding to the points in first image is drawn on second image\n",
    "\n",
    "# Find epilines corresponding to points in right image (second image)\n",
    "# drawing its lines on left image\n",
    "linesL = cv2.computeCorrespondEpilines(ptsR.reshape(-1,1,2), 2,F)\n",
    "linesL = linesL.reshape(-1,3)\n",
    "img5,img6 = drawlines(imgL,imgR,linesL,ptsL,ptsR)\n",
    "\n",
    "# Find epilines corresponding to points in left image (first image) and\n",
    "# drawing its lines on right image\n",
    "linesR = cv2.computeCorrespondEpilines(ptsL.reshape(-1,1,2), 1,F)\n",
    "linesR = linesR.reshape(-1,3)\n",
    "img3,img4 = drawlines(imgR,imgL,linesR,ptsR,ptsL)\n",
    "\n",
    "# Display\n",
    "plt.subplot(121),plt.imshow(img5),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(img3),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Dense Optical Flow: Computing the optical flow for all the points in the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Video reader\n",
    "SAMPLES_DATA_DIR = 'C:/opencv/sources/samples/data/'\n",
    "videoreader = cv2.VideoCapture(SAMPLES_DATA_DIR+\"vtest.avi\")\n",
    "\n",
    "# Read a single frame (the first frame)\n",
    "# read() also returns a bool (True/False). If frame is read correctly, it will be True\n",
    "ret, currentframeRGB = videoreader.read()\n",
    "currentframeRGB = cv2.pyrDown(currentframeRGB)\n",
    "# convert to grayscale\n",
    "currentframe = cv2.cvtColor(currentframeRGB,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# To display the flow, we'll map different flow directions into different colors\n",
    "# The flow magnitude determines the color intensity\n",
    "# Much easier to do this using HSV color space\n",
    "# Hue <- flow direction\n",
    "# Value <- flow magnitude\n",
    "# Saturation <- constant (not encoding any information in the saturation channel)\n",
    "# Initialize the hsv image\n",
    "hsv = np.zeros_like(currentframeRGB)\n",
    "# set the saturation channel to 255 for all pixels\n",
    "hsv[:,:,1] = 255\n",
    "\n",
    "while(1):\n",
    "    # Update the previous frame to the current frame\n",
    "    previousFrame = currentframe.copy()\n",
    "   \n",
    "    # Read the next frame\n",
    "    ret, currentframeRGB = videoreader.read()\n",
    "    currentframeRGB = cv2.pyrDown(currentframeRGB)\n",
    "    # convert to grayscale\n",
    "    currentframe = cv2.cvtColor(currentframeRGB,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute optical flow using the Gunner Farneback’s algorithm\n",
    "    # calcOpticalFlowFarneback(prev,next,pyr_scale,levels,winsize,iterations,poly_n,poly_sigma,flags)\n",
    "    # pyr_scale: parameter specifying the image scale (<1) to build pyramids\n",
    "    # levels: number of pyramid layers including the initial image\n",
    "    # winsize: averaging window size\n",
    "    # iterations: number of iterations the algorithm does at each pyramid level\n",
    "    # poly_n: size of the pixel neighborhood used to find polynomial expansion in each pixel\n",
    "    # poly_sigma: standard deviation of the Gaussian that is used to smooth derivatives\n",
    "    # flags: OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow approximation\n",
    "    # flow has the same size as prev with two channels: horizontal (0) and vertical (1) flow components\n",
    "    flow = cv2.calcOpticalFlowFarneback(previousFrame,currentframe, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Compute the mag/ang flow from the horizontal/vertical flow\n",
    "    mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1])\n",
    "    \n",
    "    # Set the hue to the angular flow (in radians)\n",
    "    hsv[:,:,0] = (ang * 180 /np.pi) / 2\n",
    "    \n",
    "    # Set the intensity to the normalized mag flow\n",
    "    hsv[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert to RGB for display\n",
    "    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Display\n",
    "    cv2.imshow('Original',currentframeRGB)\n",
    "    cv2.imshow('OpticalFlow',rgb)\n",
    "    # Wait for 100ms \n",
    "    k = cv2.waitKey(1000) & 0xff\n",
    "    # Terminate on pressing the 'escape' key \n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Destroy the video reader    \n",
    "videoreader.release()\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lucas-Kanade Optical Flow: Tracking conrner-like points in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Video reader\n",
    "SAMPLES_DATA_DIR = 'C:/opencv/sources/samples/data/'\n",
    "videoreader = cv2.VideoCapture(SAMPLES_DATA_DIR+\"vtest.avi\")\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 15 )\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0,255,(100,3))\n",
    "\n",
    "# Read a single frame (the first frame)\n",
    "ret, currentframeRGB = videoreader.read()\n",
    "# convert to grayscale\n",
    "currentframe = cv2.cvtColor(currentframeRGB,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find corners in the first frame\n",
    "p0 = cv2.goodFeaturesToTrack(currentframe, mask = None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(currentframeRGB)\n",
    "\n",
    "while(1):\n",
    "    # Update the previous frame to the current frame\n",
    "    previousFrame = currentframe.copy()\n",
    "   \n",
    "    # Read the next frame\n",
    "    ret, currentframeRGB = videoreader.read()    \n",
    "    # convert to grayscale\n",
    "    currentframe = cv2.cvtColor(currentframeRGB,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "    # calculate optical flow\n",
    "    # we pass the previous frame, previous points and the current frame\n",
    "    # p1: current points \n",
    "    # st: status numbers which has a value of 1 if next point is found, else zero\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(previousFrame, currentframe, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "\n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "        a,b = new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        currentframeRGB = cv2.circle(currentframeRGB,(a,b),5,color[i].tolist(),-1)\n",
    "    img = cv2.add(currentframeRGB,mask)\n",
    "\n",
    "\n",
    "     # Display\n",
    "    cv2.imshow('overlaid frame',img)\n",
    "    # Wait for 100ms \n",
    "    k = cv2.waitKey(500) & 0xff\n",
    "    # Terminate on pressing the 'escape' key \n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # Now update the previous points\n",
    "    p0 = good_new.reshape(-1,1,2)\n",
    "\n",
    "# Destroy the video reader    \n",
    "videoreader.release()\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
